DistilBERT
=====================================
epoch=3
batch_size=16
lr=2e-05

=====================================
Epochs: 1
            | Train Loss:  0.026
            | Train Accuracy:  0.810
            | Val Loss:  0.021
            | Val Accuracy:  0.854
=====================================
total_acc_val / len(dev_dataset) = 85.44, best_dev_acc = 85.44
Epochs: 2
            | Train Loss:  0.016
            | Train Accuracy:  0.899
            | Val Loss:  0.021
            | Val Accuracy:  0.897
=====================================
total_acc_val / len(dev_dataset) = 89.71, best_dev_acc = 89.71
Epochs: 3
            | Train Loss:  0.010
            | Train Accuracy:  0.937
            | Val Loss:  0.020
            | Val Accuracy:  0.879
=====================================
total_acc_val / len(dev_dataset) = 87.94, best_dev_acc = 89.71
=====================================
Total time:0:06:22.350279
Best Epoch:3
=====================================
scikit-learn Accuracy:86.76
scikit-learn Precision:86.38
scikit-learn Recall Score:86.76
scikit-learn F1 Score:86.48
